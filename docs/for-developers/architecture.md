# DataK9 Architecture

**Internal system design and implementation details** ðŸ•

This document explains how DataK9 works internally, its architecture, design patterns, and technical implementation. Like a well-trained K9 unit, DataK9's architecture is disciplined, efficient, and purpose-built for data quality vigilance.

---

## Table of Contents

1. [System Overview](#system-overview)
2. [Architecture Diagram](#architecture-diagram)
3. [Core Components](#core-components)
4. [Data Flow](#data-flow)
5. [Design Patterns](#design-patterns)
6. [Module Reference](#module-reference)
7. [Extension Points](#extension-points)
8. [Performance Characteristics](#performance-characteristics)
9. [Error Handling](#error-handling)
10. [Testing Strategy](#testing-strategy)

---

## System Overview

DataK9 is a modular, extensible Python-based system for validating data quality across multiple file formats. Like a K9 unit's systematic approach to detection, DataK9 methodically sniffs out data quality issues.

### Key Design Principles

1. **Separation of Concerns**: Each component has a single, well-defined responsibility
2. **Extensibility**: Plugin architecture allows custom validations and loaders
3. **Performance**: Chunked processing handles large files efficiently (200GB+)
4. **Configuration-Driven**: No code required for standard validations
5. **Type Safety**: Leverages Python type hints throughout
6. **Testability**: Components are loosely coupled and easily testable

### Technology Stack

- **Python 3.8+**: Core language
- **Pandas**: Data processing and manipulation
- **PyYAML**: Configuration parsing
- **pytest**: Testing framework (115+ tests, 48% coverage)
- **Jinja2**: HTML report generation

---

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DataK9 CLI Entry Point                      â”‚
â”‚                  validation_framework.cli                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Validation Engine                           â”‚
â”‚                  (core/engine.py)                                â”‚
â”‚  â€¢ Orchestrates validation workflow                              â”‚
â”‚  â€¢ Loads configuration                                           â”‚
â”‚  â€¢ Creates loaders and validations                               â”‚
â”‚  â€¢ Generates reports                                             â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                                           â”‚
      â–¼                                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Configuration      â”‚              â”‚   Validation Registry       â”‚
â”‚  (core/config.py)   â”‚              â”‚   (core/registry.py)        â”‚
â”‚  â€¢ Parses YAML      â”‚              â”‚   â€¢ Stores validation types â”‚
â”‚  â€¢ Validates config â”‚              â”‚   â€¢ Plugin registration     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                                           â”‚
      â–¼                                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Loader Factory                              â”‚
â”‚                  (loaders/factory.py)                            â”‚
â”‚  â€¢ Creates appropriate loader based on file format               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼                 â–¼            â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚CSV      â”‚    â”‚Excel     â”‚  â”‚JSON      â”‚  â”‚Parquet   â”‚  â”‚Custom    â”‚
â”‚Loader   â”‚    â”‚Loader    â”‚  â”‚Loader    â”‚  â”‚Loader    â”‚  â”‚Loaders   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚              â”‚             â”‚             â”‚             â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Data Iterator             â”‚
                    â”‚   (yields DataFrame chunks) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Validation Rules                                â”‚
â”‚                  (validations/base.py)                           â”‚
â”‚  â€¢ Abstract base class                                           â”‚
â”‚  â€¢ Condition evaluation                                          â”‚
â”‚  â€¢ Result creation                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼                 â–¼            â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚File     â”‚    â”‚Schema    â”‚  â”‚Field     â”‚  â”‚Record    â”‚  â”‚Custom    â”‚
â”‚Level    â”‚    â”‚Checks    â”‚  â”‚Checks    â”‚  â”‚Checks    â”‚  â”‚Checks    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚              â”‚             â”‚             â”‚             â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   ValidationResult          â”‚
                    â”‚   (core/results.py)         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Report Generation         â”‚
                    â”‚   (reporters/)              â”‚
                    â”‚   â€¢ HTML Reporter           â”‚
                    â”‚   â€¢ JSON Reporter           â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Core Components

### 1. Validation Engine (`core/engine.py`)

**Purpose**: Orchestrates the entire validation workflow

**Key Responsibilities**:
- Load and parse configuration
- Create data loaders for each file
- Instantiate validation rules
- Execute validations in order
- Collect and aggregate results
- Generate reports

**Key Methods**:
```python
class ValidationEngine:
    def __init__(self, config: ValidationConfig)
    def run(self, verbose: bool) -> ValidationReport
    def _validate_file(self, file_config, verbose) -> FileValidationReport
    def generate_html_report(self, report, output_path)
    def generate_json_report(self, report, output_path)
```

**Workflow**:
1. Parse configuration from YAML
2. For each file in configuration:
   - Create appropriate loader
   - Get file metadata
   - For each validation rule:
     - Instantiate validation class
     - Create data iterator
     - Execute validation
     - Collect results
3. Aggregate results
4. Generate reports

---

### 2. Configuration System (`core/config.py`)

**Purpose**: Parse and validate YAML configuration

**Key Classes**:
```python
class ValidationConfig:
    job_name: str
    description: str
    chunk_size: int
    max_sample_failures: int
    files: List[Dict[str, Any]]

    @classmethod
    def from_yaml(cls, config_path: str) -> ValidationConfig
    def to_dict(self) -> Dict[str, Any]
```

**Configuration Validation**:
- Validates required fields
- Checks file paths exist
- Validates severity levels
- Infers file formats from extensions
- Parses validation parameters

**Design Pattern**: Builder pattern for configuration construction

---

### 3. Registry System (`core/registry.py`)

**Purpose**: Plugin architecture for validation rules

**Key Features**:
- Singleton pattern for global registry
- Register validation classes by name
- Retrieve validation classes for instantiation
- List available validations

**Key Methods**:
```python
class ValidationRegistry:
    def register(self, name: str, validation_class: Type[DataValidationRule])
    def get(self, name: str) -> Type[DataValidationRule]
    def is_registered(self, name: str) -> bool
    def list_available(self) -> List[str]

# Global functions
def get_registry() -> ValidationRegistry
def register_validation(name: str, validation_class)
```

**Auto-Registration**:
Built-in validations are automatically registered on import:
```python
# validation_framework/validations/builtin/registry.py
def register_all_builtin_validations():
    register_validation("MandatoryFieldCheck", MandatoryFieldCheck)
    register_validation("RangeCheck", RangeCheck)
    # ... all built-in validations
```

---

### 4. Loader System (`loaders/`)

**Purpose**: Abstract file format differences

**Base Class** (`loaders/base.py`):
```python
class DataLoader(ABC):
    @abstractmethod
    def load(self) -> Iterator[pd.DataFrame]:
        """Return iterator yielding data chunks"""

    @abstractmethod
    def get_metadata(self) -> Dict[str, Any]:
        """Return file metadata (row count, columns, etc.)"""
```

**Loader Factory** (`loaders/factory.py`):
```python
class LoaderFactory:
    _loaders: Dict[str, Type[DataLoader]] = {
        "csv": CSVLoader,
        "excel": ExcelLoader,
        "json": JSONLoader,
        "parquet": ParquetLoader,
    }

    @classmethod
    def create_loader(cls, file_path, file_format, **kwargs) -> DataLoader

    @classmethod
    def register_loader(cls, format_name, loader_class)
```

**Design Pattern**: Factory pattern with registration mechanism

**Implementations**:
- **CSVLoader**: Uses `pd.read_csv()` with chunking
- **ExcelLoader**: Uses `pd.read_excel()` with sheet support
- **JSONLoader**: Handles JSON arrays and JSON Lines with flattening
- **ParquetLoader**: Uses `pd.read_parquet()` with columnar efficiency (10x faster!)

---

### 5. Validation Rule System (`validations/`)

**Base Class** (`validations/base.py`):
```python
class DataValidationRule(ABC):
    def __init__(self, name, severity, params, condition=None):
        self.name = name
        self.severity = severity
        self.params = params
        self.condition = condition

    @abstractmethod
    def validate(self, data_iterator, context) -> ValidationResult:
        """Execute validation logic"""

    @abstractmethod
    def get_description(self) -> str:
        """Human-readable description"""

    def _evaluate_condition(self, df) -> pd.Series:
        """Evaluate condition expression"""

    def _create_result(self, passed, message, **kwargs) -> ValidationResult:
        """Helper to create validation results"""
```

**Validation Categories**:
1. **File-Level**: Operate on entire file (EmptyFileCheck, RowCountRangeCheck)
2. **Schema**: Validate structure (SchemaMatchCheck, ColumnPresenceCheck)
3. **Field-Level**: Per-field validation (MandatoryFieldCheck, RangeCheck, RegexCheck)
4. **Record-Level**: Cross-row validation (DuplicateRowCheck, UniqueKeyCheck)
5. **Conditional**: If-then-else logic (ConditionalValidation)
6. **Advanced**: Statistical and complex (StatisticalOutlierCheck, CrossFieldComparisonCheck)

**Design Pattern**: Template Method pattern - base class defines workflow, subclasses implement specifics

---

### 6. Results System (`core/results.py`)

**Purpose**: Standardized validation results

**Key Classes**:
```python
@dataclass
class ValidationResult:
    rule_name: str
    severity: Severity
    passed: bool
    message: str
    failed_count: int
    total_count: int
    sample_failures: List[Dict]
    execution_time: float

class FileValidationReport:
    file_name: str
    file_path: str
    status: Status
    validations: List[ValidationResult]

    def add_result(self, result: ValidationResult)
    def update_status(self)

class ValidationReport:
    job_name: str
    execution_time: datetime
    overall_status: Status
    file_reports: List[FileValidationReport]

    def add_file_report(self, file_report)
    def update_overall_status(self)
    def to_dict(self) -> Dict
```

**Status Enum**:
```python
class Status(Enum):
    PASSED = "PASSED"
    FAILED = "FAILED"
    WARNING = "WARNING"

class Severity(Enum):
    ERROR = "ERROR"
    WARNING = "WARNING"
```

---

### 7. Reporter System (`reporters/`)

**Purpose**: Generate output reports

**Base Class** (`reporters/base.py`):
```python
class Reporter(ABC):
    @abstractmethod
    def generate(self, report: ValidationReport, output_path: str)
```

**Implementations**:
- **HTMLReporter** (`reporters/html_reporter.py`):
  - Uses Jinja2 templates
  - Generates styled, interactive HTML with dark theme
  - Includes summary, details, sample failures

- **JSONReporter** (`reporters/json_reporter.py`):
  - Serializes ValidationReport to JSON
  - Machine-readable for CI/CD integration
  - Preserves all validation details

---

## Data Flow

### Detailed Execution Flow

```
1. CLI Entry
   â”œâ”€> Parse command line arguments
   â””â”€> Load configuration file

2. Configuration Loading
   â”œâ”€> Parse YAML
   â”œâ”€> Validate structure
   â”œâ”€> Resolve file paths
   â””â”€> Create ValidationConfig object

3. Engine Initialization
   â”œâ”€> Create ValidationEngine(config)
   â””â”€> Get validation registry

4. For Each File:
   â”œâ”€> Create Loader
   â”‚   â”œâ”€> Detect/validate format
   â”‚   â”œâ”€> Create appropriate loader instance
   â”‚   â””â”€> Get file metadata
   â”‚
   â”œâ”€> For Each Validation:
   â”‚   â”œâ”€> Get validation class from registry
   â”‚   â”œâ”€> Instantiate with parameters
   â”‚   â”œâ”€> Create data iterator (chunked)
   â”‚   â”‚
   â”‚   â”œâ”€> Execute Validation:
   â”‚   â”‚   â”œâ”€> For each chunk:
   â”‚   â”‚   â”‚   â”œâ”€> Evaluate condition (if present)
   â”‚   â”‚   â”‚   â”œâ”€> Filter rows by condition
   â”‚   â”‚   â”‚   â”œâ”€> Apply validation logic
   â”‚   â”‚   â”‚   â”œâ”€> Collect failures
   â”‚   â”‚   â”‚   â””â”€> Continue to next chunk
   â”‚   â”‚   â”‚
   â”‚   â”‚   â””â”€> Create ValidationResult
   â”‚   â”‚
   â”‚   â””â”€> Add result to FileValidationReport
   â”‚
   â””â”€> Add FileValidationReport to ValidationReport

5. Aggregation
   â”œâ”€> Calculate overall status
   â”œâ”€> Count total errors/warnings
   â””â”€> Calculate execution time

6. Report Generation
   â”œâ”€> HTML Report (if requested)
   â””â”€> JSON Report (if requested)

7. Exit
   â””â”€> Return exit code (0=pass, 1=fail, 2=error)
```

### Data Iterator Pattern

**Why Chunked Processing?**
- Memory efficiency for large files (200GB+)
- Consistent interface across file formats
- Enables streaming validation

**Implementation**:
```python
def load(self) -> Iterator[pd.DataFrame]:
    """Yield DataFrame chunks"""
    for chunk in pd.read_csv(self.file_path, chunksize=self.chunk_size):
        yield chunk

# Usage in validation
for chunk in data_iterator:
    # Process this chunk
    for idx, row in chunk.iterrows():
        # Validate row
        pass
```

**Memory Characteristics**:
- Only `chunk_size` rows in memory at once (default: 50,000)
- Validations process incrementally
- Sample failures collected up to `max_sample_failures`
- **Example**: 50,000 rows Ã— 50 columns Ã— 8 bytes â‰ˆ 20 MB per chunk

---

## Design Patterns

### 1. Factory Pattern

**Used in**: Loader creation, Validation instantiation

**Purpose**: Create objects without specifying exact class

**Implementation**:
```python
class LoaderFactory:
    _loaders = {"csv": CSVLoader, "excel": ExcelLoader, ...}

    @classmethod
    def create_loader(cls, file_path, file_format, **kwargs):
        loader_class = cls._loaders[file_format]
        return loader_class(file_path, **kwargs)
```

**Benefits**:
- Easy to add new loaders
- Centralized creation logic
- Runtime format selection

---

### 2. Registry Pattern

**Used in**: Validation rule registration

**Purpose**: Plugin architecture for extensibility

**Implementation**:
```python
class ValidationRegistry:
    def __init__(self):
        self._validations: Dict[str, Type[DataValidationRule]] = {}

    def register(self, name, validation_class):
        self._validations[name] = validation_class

    def get(self, name):
        return self._validations[name]
```

**Benefits**:
- Decouple validation discovery from usage
- Enable custom validations
- Auto-registration on import

---

### 3. Template Method Pattern

**Used in**: DataValidationRule base class

**Purpose**: Define workflow skeleton, let subclasses fill in steps

---

### 4. Iterator Pattern

**Used in**: Data loading

**Purpose**: Sequential access without exposing underlying representation

**Benefits**:
- Uniform interface for all file formats
- Memory-efficient streaming
- Lazy evaluation

---

### 5. Strategy Pattern

**Used in**: Conditional validation, severity handling

**Purpose**: Select algorithm at runtime

---

### 6. Singleton Pattern

**Used in**: ValidationRegistry

**Purpose**: Single global registry instance

**Implementation**:
```python
_global_registry = None

def get_registry() -> ValidationRegistry:
    global _global_registry
    if _global_registry is None:
        _global_registry = ValidationRegistry()
    return _global_registry
```

---

## Module Reference

### Core Modules

```
validation_framework/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ cli.py                    # Command-line interface
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ engine.py             # ValidationEngine - main orchestrator
â”‚   â”œâ”€â”€ config.py             # ValidationConfig - YAML parsing
â”‚   â”œâ”€â”€ registry.py           # ValidationRegistry - plugin system
â”‚   â”œâ”€â”€ results.py            # ValidationResult, Report classes
â”‚   â””â”€â”€ logging_config.py     # Logging configuration
â”œâ”€â”€ loaders/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py               # DataLoader abstract base
â”‚   â”œâ”€â”€ factory.py            # LoaderFactory
â”‚   â”œâ”€â”€ csv_loader.py         # CSV file support
â”‚   â”œâ”€â”€ excel_loader.py       # Excel file support
â”‚   â”œâ”€â”€ json_loader.py        # JSON file support
â”‚   â””â”€â”€ parquet_loader.py     # Parquet file support
â”œâ”€â”€ validations/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py               # DataValidationRule base class
â”‚   â”œâ”€â”€ builtin/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ registry.py       # Auto-registration
â”‚   â”‚   â”œâ”€â”€ file_checks.py    # File-level validations
â”‚   â”‚   â”œâ”€â”€ schema_checks.py  # Schema validations
â”‚   â”‚   â”œâ”€â”€ field_checks.py   # Field-level validations
â”‚   â”‚   â”œâ”€â”€ record_checks.py  # Record-level validations
â”‚   â”‚   â”œâ”€â”€ conditional.py    # Conditional validation
â”‚   â”‚   â”œâ”€â”€ inline_checks.py  # Inline/custom checks
â”‚   â”‚   â””â”€â”€ advanced_checks.py # Statistical validations
â”‚   â””â”€â”€ custom/
â”‚       â””â”€â”€ __init__.py       # Custom validations
â””â”€â”€ reporters/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ base.py               # Reporter abstract base
    â”œâ”€â”€ html_reporter.py      # HTML report generation
    â””â”€â”€ json_reporter.py      # JSON report generation
```

### Dependencies

**Core Dependencies**:
- `pandas >= 1.3.0` - Data manipulation
- `pyyaml >= 5.4.0` - YAML parsing
- `jinja2 >= 3.0.0` - HTML templating

**Optional Dependencies**:
- `openpyxl >= 3.0.0` - Excel support
- `pyarrow >= 5.0.0` - Parquet support (10x faster!)
- `colorama >= 0.4.0` - Colored console output

---

## Extension Points

### 1. Custom Data Loaders

**Create a custom loader for new file formats**:

```python
from validation_framework.loaders.base import DataLoader
from validation_framework.loaders.factory import LoaderFactory
import pandas as pd

class XMLLoader(DataLoader):
    def load(self) -> Iterator[pd.DataFrame]:
        # Custom XML parsing logic
        # Yield DataFrame chunks
        pass

    def get_metadata(self) -> Dict[str, Any]:
        # Return file metadata
        pass

# Register the loader
LoaderFactory.register_loader("xml", XMLLoader)
```

**See:** [Custom Loaders Guide](custom-loaders.md)

---

### 2. Custom Validations

**Create custom validation rules**:

```python
from validation_framework.validations.base import DataValidationRule, ValidationResult
from validation_framework.core.registry import register_validation

class MyCustomValidation(DataValidationRule):
    def get_description(self) -> str:
        return "My custom validation logic"

    def validate(self, data_iterator, context) -> ValidationResult:
        # Custom validation logic
        # Process chunks
        # Return ValidationResult
        pass

# Register the validation
register_validation("MyCustomValidation", MyCustomValidation)
```

**See:** [Custom Validations Guide](custom-validations.md)

---

### 3. Custom Reporters

**Create custom report formats**:

```python
from validation_framework.reporters.base import Reporter
from validation_framework.core.results import ValidationReport

class PDFReporter(Reporter):
    def generate(self, report: ValidationReport, output_path: str):
        # Generate PDF report
        pass
```

**See:** [Custom Reporters Guide](custom-reporters.md)

---

## Performance Characteristics

### Time Complexity

**Per Validation**:
- File-level checks: O(1) - single pass
- Schema checks: O(1) - metadata only
- Field checks: O(n) - one pass through data
- Duplicate checks: O(n log n) - requires sorting/hashing
- Statistical checks: O(n) to O(2n) - may require multiple passes

**Overall**: O(v * n) where v = number of validations, n = number of rows

### Space Complexity

**Memory Usage**:
- Chunk size: `chunk_size * number_of_columns * 8 bytes` (approximate)
- Sample failures: `max_sample_failures * number_of_failed_validations`
- Metadata: O(1) per file

**Example**:
- 50,000 rows/chunk Ã— 50 columns Ã— 8 bytes â‰ˆ 20 MB per chunk
- 100 sample failures Ã— 10 validations = up to 1000 failure records

### Scalability

**File Size**:
- âœ… 1 MB - 100 MB: Excellent performance (<1-10 seconds)
- âœ… 100 MB - 1 GB: Good performance (10-120 seconds)
- âœ… 1 GB - 10 GB: Acceptable performance with tuning (2-20 minutes)
- âœ… 10 GB - 200 GB: Use Parquet format, increase chunk size (20 minutes - 4 hours)

**Number of Validations**:
- Linear scaling: 10 validations â‰ˆ 10x time of 1 validation
- Consider disabling expensive validations in development

**Optimization Strategies**:
1. Increase chunk_size for more columns/memory (up to 200,000)
2. Decrease chunk_size for less memory (down to 10,000)
3. **Use Parquet for large files (10x faster than CSV)**
4. Limit sample failures for faster reporting (10-50)
5. Order validations by speed (fast first)
6. Disable statistical checks during development

**See:** [Performance Tuning Guide](../using-datak9/performance-tuning.md)

---

## Error Handling

### Error Categories

**1. Configuration Errors** (exit code 2):
- Invalid YAML syntax
- Missing required fields
- File not found
- Invalid parameters

**2. Validation Failures** (exit code 1):
- Data quality issues (DataK9 detected problems!)
- Business rule violations
- Returns ValidationResult with passed=False

**3. Runtime Errors** (exit code 2):
- File read errors
- Memory errors
- Unexpected exceptions

### Error Handling Strategy

```python
try:
    # Validation logic
    result = validation.validate(data_iterator, context)
except FileNotFoundError:
    # Handle missing files
    logger.error("File not found")
    # Return error result
except pd.errors.ParserError:
    # Handle parsing errors
    logger.error("Cannot parse file")
except Exception as e:
    # Catch-all for unexpected errors
    logger.exception("Unexpected error")
    # Return error result
```

**Graceful Degradation**:
- Individual validation failures don't stop execution
- Errors are captured as ValidationResult objects
- Full report generated even with some failures

---

## Testing Strategy

### Test Pyramid

```
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚   Manual    â”‚  End-to-end scenarios
       â”‚   Testing   â”‚  (exploratory)
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Integration  â”‚  Full workflows
      â”‚     Tests     â”‚  (pytest)
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Component Tests  â”‚  Individual validations
    â”‚    (Unit Tests)    â”‚  (pytest - 115+ tests)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚     Smoke Tests        â”‚  Quick sanity checks
  â”‚  (Built-in fixtures)   â”‚  (pytest)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Test Categories

**1. Unit Tests** (validation_framework/tests/):
- Test individual validation classes
- Mock data iterators
- Verify ValidationResult correctness

**2. Integration Tests**:
- Test full validation workflows
- Use actual test data files
- Verify end-to-end results

**3. Fixture Tests**:
- Test with sample data files
- Verify file format handling
- Test edge cases

**Test Coverage**:
- Current: 48% code coverage
- Target: 80%+ code coverage
- Critical paths: 100% coverage
- Edge cases: Well-documented

**See:** [Testing Guide](testing-guide.md)

---

## Next Steps

- **[Custom Validations](custom-validations.md)** - Create custom validation rules
- **[Custom Loaders](custom-loaders.md)** - Add new file format support
- **[Custom Reporters](custom-reporters.md)** - Generate custom report formats
- **[API Reference](api-reference.md)** - Complete Python API documentation
- **[Contributing](contributing.md)** - Contribution guidelines

---

**ðŸ• Guard your data with DataK9 - architected for data quality vigilance!**
