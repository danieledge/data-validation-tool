# Performance Tuning Guide

**Optimize DataK9 for Speed and Scale**

DataK9 is built for performance, but you can tune it further for your specific workloads. This guide shows you how to optimize validation speed and memory usage.

---

## Table of Contents

1. [Performance Overview](#performance-overview)
2. [Chunk Size Optimization](#chunk-size-optimization)
3. [File Format Selection](#file-format-selection)
4. [Validation Ordering](#validation-ordering)
5. [Memory Management](#memory-management)
6. [Parallel Processing](#parallel-processing)
7. [Database Optimization](#database-optimization)
8. [Benchmarking](#benchmarking)
9. [Troubleshooting Slow Validations](#troubleshooting-slow-validations)

---

## Performance Overview

### DataK9 Performance Characteristics

**Out-of-the-box performance:**

| File Size | Format | Processing Time | Memory Usage |
|-----------|--------|-----------------|--------------|
| 1 MB | CSV | < 1 second | ~50 MB |
| 100 MB | CSV | ~10 seconds | ~200 MB |
| 1 GB | CSV | ~2 minutes | ~400 MB |
| 10 GB | Parquet | ~5 minutes | ~400 MB |
| 100 GB | Parquet | ~45 minutes | ~400 MB |
| 200 GB | Parquet | ~4 hours | ~400 MB |

**Key Insight:** Memory usage stays constant regardless of file size due to chunked processing.

### Performance Factors

**1. File Format** (biggest impact)
- Parquet: 10x faster than CSV
- CSV with compression: Slower
- JSON: Moderate

**2. Chunk Size** (memory/speed tradeoff)
- Larger chunks: Faster, more memory
- Smaller chunks: Slower, less memory

**3. Validation Complexity** (varies)
- Simple checks: Fast (RegexCheck, RangeCheck)
- Complex checks: Slower (StatisticalOutlierCheck, CrossFileCheck)

**4. Data Characteristics**
- Row count
- Column count
- Value distributions
- Null percentages

---

## Chunk Size Optimization

### What is Chunk Size?

DataK9 processes data in chunks to manage memory:

```python
# Default chunk size: 50,000 rows
chunk_size = 50000

# File has 10 million rows
# Processed in 200 chunks (10M / 50K)
```

### Choosing Chunk Size

**Default (50,000 rows):**
- Good for most use cases
- Balances speed and memory
- Works with files up to 200GB

**Larger Chunks (100,000 - 500,000):**
- Faster processing
- More memory required
- Better for:
  - High RAM systems (16GB+)
  - Fewer columns
  - Simple validations

**Smaller Chunks (10,000 - 25,000):**
- Lower memory usage
- Slower processing
- Better for:
  - Low RAM systems (<8GB)
  - Many columns
  - Complex validations

**Formula:**

```python
# Estimate memory per chunk
memory_mb = chunk_size √ó column_count √ó 8 bytes / 1MB

# Example: 50,000 rows √ó 50 columns √ó 8 bytes
memory_mb = 50000 √ó 50 √ó 8 / 1024 / 1024 ‚âà 19 MB
```

### Setting Chunk Size

**In YAML config:**

```yaml
settings:
  chunk_size: 100000  # Increase for speed

  # or

  chunk_size: 25000   # Decrease for memory
```

**Impact Example:**

```
File: 10GB, 10 million rows, 50 columns

Chunk Size 25,000:
- Chunks: 400
- Memory: ~10 MB per chunk
- Time: ~15 minutes

Chunk Size 50,000 (default):
- Chunks: 200
- Memory: ~20 MB per chunk
- Time: ~10 minutes

Chunk Size 100,000:
- Chunks: 100
- Memory: ~40 MB per chunk
- Time: ~7 minutes
```

### Finding Optimal Chunk Size

**Test with your data:**

```bash
# Test different chunk sizes
python3 -m validation_framework.cli validate config.yaml  # Default 50K

# Edit config, set chunk_size: 100000
python3 -m validation_framework.cli validate config.yaml  # 100K

# Edit config, set chunk_size: 25000
python3 -m validation_framework.cli validate config.yaml  # 25K

# Compare times and memory usage
```

**Monitor memory:**

```bash
# On Linux
htop  # Watch memory usage during validation

# On Mac
Activity Monitor

# On Windows
Task Manager ‚Üí Performance ‚Üí Memory
```

---

## File Format Selection

### Format Performance Comparison

| Format | Read Speed | Write Speed | Compression | Schema | Best For |
|--------|------------|-------------|-------------|--------|----------|
| **Parquet** | ‚ö°‚ö°‚ö° Fastest | ‚ö°‚ö° Fast | ‚úÖ Built-in | ‚úÖ Yes | Large files, analytics |
| **CSV** | ‚ö° Slow | ‚ö°‚ö°‚ö° Fastest | ‚ùå External | ‚ùå No | Small files, compatibility |
| **JSON** | ‚ö° Slow | ‚ö°‚ö° Fast | ‚ùå External | ‚úÖ Yes | APIs, nested data |
| **Excel** | ‚ö° Slowest | ‚ö° Slow | ‚úÖ Built-in | ‚ö° Partial | Business users, small files |

### When to Use Parquet

**Convert CSV to Parquet for:**
- Files > 1 GB
- Repeated validations
- Many columns
- Numeric-heavy data
- Analytics workloads

**Speed Improvement:**

```
CSV (10GB):  ~20 minutes
Parquet (10GB): ~5 minutes

CSV (100GB): ~3 hours
Parquet (100GB): ~45 minutes
```

**Conversion:**

```python
import pandas as pd

# Convert CSV to Parquet
df = pd.read_csv('large_file.csv')
df.to_parquet('large_file.parquet', compression='snappy')

# Then validate Parquet
python3 -m validation_framework.cli validate config.yaml
```

### CSV Optimization

If you must use CSV:

**1. Use Appropriate Delimiter:**
```yaml
files:
  - name: "data"
    path: "data.csv"
    delimiter: "|"  # Pipe-delimited faster than comma for numeric data
```

**2. Skip Compression:**
```bash
# Uncompressed CSV faster than gzipped
gunzip large_file.csv.gz
python3 -m validation_framework.cli validate config.yaml
```

**3. Remove Unnecessary Columns:**
```python
# Only include columns you're validating
df = pd.read_csv('large_file.csv', usecols=['id', 'email', 'age'])
df.to_csv('filtered_file.csv', index=False)
```

---

## Validation Ordering

### Order Matters

**Fail Fast Principle:**

Run cheap, high-impact validations first:

```yaml
validations:
  # 1. File-level (milliseconds)
  - type: "EmptyFileCheck"
    severity: "ERROR"

  # 2. Schema (seconds)
  - type: "ColumnPresenceCheck"
    severity: "ERROR"

  # 3. Mandatory fields (seconds)
  - type: "MandatoryFieldCheck"
    severity: "ERROR"

  # 4. Field formats (seconds-minutes)
  - type: "RegexCheck"
    severity: "ERROR"

  # 5. Statistical (minutes)
  - type: "StatisticalOutlierCheck"
    severity: "WARNING"

  # 6. Cross-file (minutes)
  - type: "ReferentialIntegrityCheck"
    severity: "ERROR"
```

**Why?**

If file is empty, no point running expensive statistical checks.

### Validation Performance

**Fast Validations** (milliseconds-seconds):
- EmptyFileCheck
- RowCountRangeCheck
- ColumnPresenceCheck
- MandatoryFieldCheck (if few fields)

**Medium Validations** (seconds-minutes):
- RegexCheck
- ValidValuesCheck
- RangeCheck
- UniqueKeyCheck
- DuplicateRowCheck

**Slow Validations** (minutes):
- StatisticalOutlierCheck
- DistributionCheck
- CorrelationCheck
- CrossFileComparisonCheck
- BaselineComparisonCheck

**Order Example:**

```yaml
# ‚ùå BAD ORDER
validations:
  - type: "StatisticalOutlierCheck"  # Slow - runs even if file empty
  - type: "CrossFileComparisonCheck" # Slow
  - type: "EmptyFileCheck"           # Fast - should be first!

# ‚úÖ GOOD ORDER
validations:
  - type: "EmptyFileCheck"           # Fast - fail immediately if empty
  - type: "MandatoryFieldCheck"      # Fast - check critical fields
  - type: "StatisticalOutlierCheck"  # Slow - run only if basics pass
  - type: "CrossFileComparisonCheck" # Slow - run last
```

---

## Memory Management

### Memory Architecture

DataK9 uses memory efficiently:

```
Total Memory =
  Base overhead (~100 MB) +
  Chunk memory (chunk_size √ó columns √ó 8 bytes) +
  Validation state (~50 MB) +
  Sample failures (max_sample_failures √ó failure_size)
```

### Reducing Memory Usage

**1. Reduce Chunk Size:**

```yaml
settings:
  chunk_size: 25000  # Half default ‚Üí half memory
```

**2. Limit Sample Failures:**

```yaml
settings:
  max_sample_failures: 50  # Default 100
```

**3. Process Files Sequentially:**

Instead of validating multiple files in one job:

```bash
# ‚ùå High memory: Validates all 10 files together
python3 -m validation_framework.cli validate all_files.yaml

# ‚úÖ Low memory: Validates files one at a time
python3 -m validation_framework.cli validate file1.yaml
python3 -m validation_framework.cli validate file2.yaml
# ... repeat for each file
```

**4. Use Parquet:**

Parquet's columnar format allows reading only needed columns:

```yaml
files:
  - name: "data"
    path: "data.parquet"
    columns:  # Only load these columns
      - "customer_id"
      - "email"
      - "age"
```

### Memory Monitoring

**Check current usage:**

```python
import psutil

# During validation
process = psutil.Process()
print(f"Memory: {process.memory_info().rss / 1024 / 1024:.2f} MB")
```

**Set memory limits (Linux):**

```bash
# Limit to 2GB
ulimit -v 2097152  # 2GB in KB

python3 -m validation_framework.cli validate config.yaml
```

---

## Parallel Processing

### Current Limitations

DataK9 v1.x processes validations sequentially:

```
File 1: Validation A ‚Üí Validation B ‚Üí Validation C
File 2: Validation A ‚Üí Validation B ‚Üí Validation C
```

### Workarounds for Parallelism

**1. Parallel File Processing:**

```bash
# Process 4 files in parallel
python3 -m validation_framework.cli validate file1.yaml &
python3 -m validation_framework.cli validate file2.yaml &
python3 -m validation_framework.cli validate file3.yaml &
python3 -m validation_framework.cli validate file4.yaml &

wait  # Wait for all to complete
```

**2. GNU Parallel:**

```bash
# Create file list
ls *.yaml > configs.txt

# Process in parallel (4 jobs at once)
cat configs.txt | parallel -j 4 'python3 -m validation_framework.cli validate {}'
```

**3. Task Schedulers:**

```python
# Airflow DAG with parallel tasks
from airflow import DAG
from airflow.operators.bash_operator import BashOperator

dag = DAG('data_validation', schedule_interval='@daily')

validate_customers = BashOperator(
    task_id='validate_customers',
    bash_command='python3 -m validation_framework.cli validate customers.yaml',
    dag=dag
)

validate_orders = BashOperator(
    task_id='validate_orders',
    bash_command='python3 -m validation_framework.cli validate orders.yaml',
    dag=dag
)

# Run in parallel
[validate_customers, validate_orders]
```

### Future Enhancement

DataK9 v2.x will support built-in parallelism:

```yaml
# Future feature
settings:
  parallel_validations: true
  max_workers: 4
```

---

## Database Optimization

### Database Performance Tips

**1. Use Indexes:**

```sql
-- Create indexes on join columns
CREATE INDEX idx_customer_id ON customers(customer_id);
CREATE INDEX idx_order_customer ON orders(customer_id);
```

**2. Optimize Queries:**

```yaml
# ‚ùå BAD: Scans entire table
validations:
  - type: "SQLCustomCheck"
    params:
      query: "SELECT * FROM customers WHERE status = 'active'"

# ‚úÖ GOOD: Uses index, limits columns
validations:
  - type: "SQLCustomCheck"
    params:
      query: "SELECT customer_id, email FROM customers WHERE status = 'active' INDEX(idx_status)"
```

**3. Use Connection Pooling:**

```yaml
files:
  - name: "customer_data"
    format: "database"
    connection_string: "postgresql://user:pass@localhost/db?pool_size=10"
```

**4. Batch Referential Checks:**

```yaml
# Instead of checking each row individually
# DataK9 batches lookups for efficiency
validations:
  - type: "ReferentialIntegrityCheck"
    params:
      field: "customer_id"
      reference_file: "customers"  # Loaded once, cached
      reference_field: "customer_id"
```

---

## Benchmarking

### Measure Performance

**1. Time Individual Validations:**

```bash
# Run with verbose logging
python3 -m validation_framework.cli validate config.yaml --verbose

# Output shows time per validation:
# ‚úÖ EmptyFileCheck: 0.01s
# ‚úÖ MandatoryFieldCheck: 1.2s
# ‚ùå RegexCheck: 15.3s  ‚Üê Slow!
# ‚úÖ RangeCheck: 2.1s
```

**2. Profile with Time:**

```bash
time python3 -m validation_framework.cli validate config.yaml

# Output:
# real    2m15.432s
# user    2m10.123s
# sys     0m3.245s
```

**3. Compare Configurations:**

```bash
# Baseline
time python3 -m validation_framework.cli validate baseline.yaml

# Optimized (larger chunks)
time python3 -m validation_framework.cli validate optimized.yaml

# Compare times
```

### Benchmark Script

```bash
#!/bin/bash
# benchmark.sh - Compare different configurations

echo "=== DataK9 Performance Benchmark ==="

for CHUNK_SIZE in 25000 50000 100000 200000; do
    echo "\nTesting chunk_size=$CHUNK_SIZE"

    # Update config
    sed -i "s/chunk_size:.*/chunk_size: $CHUNK_SIZE/" config.yaml

    # Measure time
    START=$(date +%s)
    python3 -m validation_framework.cli validate config.yaml > /dev/null
    END=$(date +%s)

    DURATION=$((END - START))
    echo "Duration: ${DURATION}s"
done
```

---

## Troubleshooting Slow Validations

### Diagnostic Steps

**1. Identify Slow Validations:**

```bash
# Run with verbose logging
python3 -m validation_framework.cli validate config.yaml --verbose

# Look for validations taking >10s
```

**2. Profile Specific Validation:**

```python
import cProfile
import pstats

# Profile validation
profiler = cProfile.Profile()
profiler.enable()

# Run validation
result = validation.validate(data_iterator, context)

profiler.disable()
stats = pstats.Stats(profiler)
stats.sort_stats('cumulative')
stats.print_stats(10)  # Top 10 slow functions
```

**3. Check Data Characteristics:**

```bash
# Profile data to understand characteristics
python3 -m validation_framework.cli profile data.csv

# Look for:
# - High row count
# - Many columns
# - High cardinality fields
# - Large string fields
```

### Common Slow Validations

**RegexCheck on Large Strings:**

```yaml
# ‚ùå SLOW: Regex on 10,000-character text fields
- type: "RegexCheck"
  params:
    field: "description"  # Long text field
    pattern: ".*keyword.*"

# ‚úÖ FASTER: Limit to shorter fields or use simpler check
- type: "StringLengthCheck"
  params:
    field: "description"
    max_length: 1000
```

**UniqueKeyCheck on High Cardinality:**

```yaml
# ‚ùå SLOW: Checking uniqueness of 10 million rows
- type: "UniqueKeyCheck"
  params:
    key_fields: ["customer_id"]

# ‚úÖ FASTER: Sample-based uniqueness check
- type: "UniqueKeyCheck"
  params:
    key_fields: ["customer_id"]
    sample_size: 100000  # Check sample instead
```

**CrossFileComparisonCheck:**

```yaml
# ‚ùå SLOW: Comparing every row across files
- type: "CrossFileComparisonCheck"

# ‚úÖ FASTER: Use database join or indexed lookup
# Pre-join files before validation
```

### Optimization Strategies

**Strategy 1: Sampling**

For statistical validations on very large files:

```yaml
validations:
  - type: "StatisticalOutlierCheck"
    params:
      field: "amount"
      sample_size: 100000  # Sample instead of full dataset
```

**Strategy 2: Caching**

For repeated validations:

```python
# Cache lookup data
@lru_cache(maxsize=10000)
def lookup_customer(customer_id):
    return customer_db.get(customer_id)
```

**Strategy 3: Conditional Validation**

Only validate when needed:

```yaml
# Don't check inactive customers
validations:
  - type: "EmailCheck"
    condition: "status == 'active'"  # Skip 70% of rows
```

---

## Performance Checklist

### Before Optimizing

- [ ] Profile your data
- [ ] Identify slow validations
- [ ] Measure baseline performance
- [ ] Set performance goals

### Quick Wins

- [ ] Use Parquet instead of CSV (10x faster)
- [ ] Order validations (fail fast)
- [ ] Increase chunk_size if memory available
- [ ] Remove unnecessary validations
- [ ] Use conditions to skip rows

### Advanced Optimization

- [ ] Parallel file processing
- [ ] Database indexing
- [ ] Sampling for statistical checks
- [ ] Custom validation optimization
- [ ] Infrastructure upgrades (SSD, RAM, CPU)

### Monitoring

- [ ] Track validation times
- [ ] Monitor memory usage
- [ ] Set up alerting for slow runs
- [ ] Regular benchmarking

---

## Next Steps

**You've learned performance tuning! Now:**

1. **[Large Files Guide](large-files.md)** - Handle 200GB+ files
2. **[Configuration Guide](configuration-guide.md)** - Optimize your config
3. **[Best Practices](best-practices.md)** - Production deployment
4. **[Troubleshooting](troubleshooting.md)** - Solve performance issues

---

**üêï Fast validation, reliable results - DataK9 guards efficiently**
